{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "YWBAT\n",
    "* explain the different parts of the sigmoid curve\n",
    "* explain how thresholds affect model outcomes/scores\n",
    "* explain how confusion matrices are helpful\n",
    "\n",
    "\n",
    "# Outline\n",
    "* load in some data\n",
    "* do some feature engineering/eda\n",
    "* build a logreg model, score it, confusion matrix it\n",
    "* make this into a workflow\n",
    "* discuss stuff\n",
    "* tune logistic regression hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/SPAM text message 20170820 - Data.csv\")\n",
    "# rename columns to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make some new features on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a column that contains an integer of the word count of the message column\n",
    "Store it to a column called `wordcount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "def count_words(message):\n",
    "    \"\"\"\n",
    "    input\n",
    "    message: str, some text message\n",
    "    \n",
    "    return\n",
    "    word_count, int, number of words in message\n",
    "    \"\"\"\n",
    "    word_count = len(message.split(\" \"))\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['message'].apply(count_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the following words from each message (upper and lower case versions)\n",
    "Store the cleaned message to a column called `cleaned message`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['the', 'a', 'as', 'be', 'that', 'this']\n",
    "\n",
    "# code here\n",
    "\n",
    "def clean_message(message, stop_words=stop_words):\n",
    "    \"\"\"\n",
    "    input\n",
    "    message: str, some text message\n",
    "    \n",
    "    return\n",
    "    cleaned_message: str, message with specific words to remove\n",
    "    \"\"\"\n",
    "    words = message.split(\" \")\n",
    "    clean_message = len([word for word in words if word.lower() not in stop_words])\n",
    "    return clean_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take a wordcount of the cleaned message\n",
    "Store this to a column called `cleaned_wordcount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "df['cleaned_wordcount'] = df['message'].apply(clean_message)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contains_stopword'] = df['word_count'] != df['cleaned_wordcount']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot a bar chart of the word count by spam/ham\n",
    "Use whichever plotting tool you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "df_grouped_by_word_count = df.groupby('category')[['word_count']].agg(np.mean).reset_index()\n",
    "\n",
    "fig = px.bar(data_frame=df_grouped_by_word_count,\n",
    "             x='category',\n",
    "             y = 'word_count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot a bar chart of the cleaned_wordcount by spam/ham\n",
    "Use whichever plotting tool you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "# code here\n",
    "df_grouped_by_cleaned_wordcount = df.groupby('category')[['cleaned_wordcount']].agg(np.mean).reset_index()\n",
    "\n",
    "fig = px.bar(data_frame=df_grouped_by_cleaned_wordcount,\n",
    "             x='category',\n",
    "             y = 'cleaned_wordcount')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count_z'] = (df['word_count'] - df['word_count'].mean()) / df['word_count'].std()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new column called `target` \n",
    "where target = 1 if category = spam else target = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "df['target'] = df['category'].apply(lambda x: 1 if x.lower().strip(\" \")=='spam' else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a scatter plot of word count vs target\n",
    "Use whichever plotting tool you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "fig = px.scatter(data_frame=df,\n",
    "                 x='word_count',\n",
    "                 y = 'target')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(data_frame=df,\n",
    "                x='category',\n",
    "                y='word_count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what do you notice in the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression would be horrible. There's 100% overlap of our SPAM values on our HAM values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Logistic Regression in sklearn using a train/test split of 0.8/0.2\n",
    "X should contain only word_count data\n",
    "\n",
    "y should be the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['word_count_z']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "logreg = LogisticRegression(fit_intercept=False)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score your model\n",
    "score it on both the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "logreg.score(X_train, y_train), logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at your model's beta coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "logreg.coef_, logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot a confusion matrix and evaluate it\n",
    "Make sure you label true/predicted axes\n",
    "\n",
    "In your evaluation mention the False Positive Rate and False Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='0.4g', cmap=sns.color_palette('Blues'))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model sucks, it predicts everything as HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which texts were mislabeled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_probs = logreg.predict_proba(X_test)[:, 1]\n",
    "y_test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_mislabeled = np.where(y_test_probs > 0.5)\n",
    "index_of_mislabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_of_mislabeled = X_test[index_of_mislabeled]\n",
    "word_count_of_mislabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sigmoid_values'] = logreg.predict_proba(X.values.reshape(-1, 1))[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['word_count_z'], y=df['target'],\n",
    "    name='Actual',\n",
    "    mode='markers',\n",
    "    marker_color='rgba(152, 0, 0, .8)'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['word_count_z'], y=df['sigmoid_values'],\n",
    "    name='Predicted',\n",
    "    mode='markers',\n",
    "    marker_color='rgba(255, 182, 193, .9)'\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did we learn?\n",
    "* Plotting 2 scatter plots on the same figure in plotly\n",
    "* `.predict_prob()` method in sklearn\n",
    "* Why `y` is between 0 and 1 in a sigmoid function\n",
    "* Preview into working with text data \n",
    "* `.values` on a pandas series\n",
    "* importance of confusion matrix in determining model quality\n",
    "* don't chase good score, check model with confusion matrix, roc/auc curve\n",
    "* reshape\n",
    "* Use seaborn heatmap for confusion matrix plotting\n",
    "* elementary version of creating numerical features from text data\n",
    "* you have to turn text data into numerical data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flatiron-env] *",
   "language": "python",
   "name": "conda-env-flatiron-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
